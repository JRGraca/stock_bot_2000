{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as pta\n",
    "import ta\n",
    "# from datapackage import Package\n",
    "# from selenium import webdriver\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import yfinance as yf\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas_ta reference\n",
    "\n",
    "pta_reference = {\n",
    "\t\"ad\": \"Accumulation/Distribution Index \",\n",
    "\t\"adx\" : \"Average Directional Movement\",\n",
    "\t\"aroon\" : \"Aroon Oscillator\",\n",
    "\t\"bbands\" : \"Bollinger bands\",\n",
    "\t\"macd\" : \"Moving Average Convergence Divergence\",\n",
    "\t\"obv\" : \"On-Balance Volume\",\n",
    "\t\"rsi\" : \"Relative Strength Index\",\n",
    "\t\"sma\" : \"Simple Moving Average\",\n",
    "\t\"stoch\" : \"Stochastic Oscillator\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Indicator testing\n",
    "\n",
    "# noc = pd.read_csv(\"test_data/NOC.csv\")\n",
    "# pta.stoch(high=noc[\"High\"], low=noc[\"Low\"], close=noc[\"Close\"]).iloc[:,0]\n",
    "# help(pta.stoch)\n",
    "# pta.adx(high=noc[\"High\"], low=noc[\"Low\"], close = noc[\"Close\"], length=30).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects the desired number of random stocks so they can be used to perform tests\n",
    "# Selected tables will be copied to the test_data folder, which is emptied beforehand\n",
    "\n",
    "n_files = 10\n",
    "sample_files = random.choices(os.listdir(\"data\"), k=n_files)\n",
    "for i in os.listdir(\"test_data\"):\n",
    "\tos.remove(\"test_data/\" + i)\n",
    "for file in sample_files:\n",
    "\tshutil.copy2(\"data/\" + file, \"test_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicates the big_table creating process on a much smaller scale\n",
    "# (also doesn't save it to file)\n",
    "\n",
    "test_table = pd.DataFrame(columns=[\"Date\", \"Close\", \"Volume\", \"Code\"])\n",
    "for i in os.listdir(\"test_data\"):\n",
    "\tlittle_table = pd.read_csv(\"test_data/\" + i)\n",
    "\tlittle_table['Code'] = i.replace(\".csv\", \"\")\n",
    "\ttest_table = pd.concat([test_table, little_table[[\"Date\", \"Close\", \"Volume\", \"Code\"]]], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records processed of 12551\n",
      "1000 records processed of 12551\n",
      "2000 records processed of 12551\n",
      "3000 records processed of 12551\n",
      "4000 records processed of 12551\n",
      "5000 records processed of 12551\n",
      "6000 records processed of 12551\n",
      "7000 records processed of 12551\n",
      "8000 records processed of 12551\n",
      "9000 records processed of 12551\n",
      "10000 records processed of 12551\n",
      "11000 records processed of 12551\n",
      "12000 records processed of 12551\n",
      "0 records processed of 12551\n",
      "1000 records processed of 12551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 records processed of 12551\n",
      "3000 records processed of 12551\n",
      "4000 records processed of 12551\n",
      "5000 records processed of 12551\n",
      "6000 records processed of 12551\n",
      "7000 records processed of 12551\n",
      "8000 records processed of 12551\n",
      "9000 records processed of 12551\n",
      "10000 records processed of 12551\n",
      "11000 records processed of 12551\n",
      "12000 records processed of 12551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records processed of 12551\n",
      "1000 records processed of 12551\n",
      "2000 records processed of 12551\n",
      "3000 records processed of 12551\n",
      "4000 records processed of 12551\n",
      "5000 records processed of 12551\n",
      "6000 records processed of 12551\n",
      "7000 records processed of 12551\n",
      "8000 records processed of 12551\n",
      "9000 records processed of 12551\n",
      "10000 records processed of 12551\n",
      "11000 records processed of 12551\n",
      "12000 records processed of 12551\n",
      "0 records processed of 3276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 records processed of 3276\n",
      "2000 records processed of 3276\n",
      "3000 records processed of 3276\n",
      "0 records processed of 7809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 records processed of 7809\n",
      "2000 records processed of 7809\n",
      "3000 records processed of 7809\n",
      "4000 records processed of 7809\n",
      "5000 records processed of 7809\n",
      "6000 records processed of 7809\n",
      "7000 records processed of 7809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records processed of 10765\n",
      "1000 records processed of 10765\n",
      "2000 records processed of 10765\n",
      "3000 records processed of 10765\n",
      "4000 records processed of 10765\n",
      "5000 records processed of 10765\n",
      "6000 records processed of 10765\n",
      "7000 records processed of 10765\n",
      "8000 records processed of 10765\n",
      "9000 records processed of 10765\n",
      "10000 records processed of 10765\n",
      "0 records processed of 4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 records processed of 4517\n",
      "2000 records processed of 4517\n",
      "3000 records processed of 4517\n",
      "4000 records processed of 4517\n",
      "0 records processed of 5023\n",
      "1000 records processed of 5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 records processed of 5023\n",
      "3000 records processed of 5023\n",
      "4000 records processed of 5023\n",
      "5000 records processed of 5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records processed of 12731\n",
      "1000 records processed of 12731\n",
      "2000 records processed of 12731\n",
      "3000 records processed of 12731\n",
      "4000 records processed of 12731\n",
      "5000 records processed of 12731\n",
      "6000 records processed of 12731\n",
      "7000 records processed of 12731\n",
      "8000 records processed of 12731\n",
      "9000 records processed of 12731\n",
      "10000 records processed of 12731\n",
      "11000 records processed of 12731\n",
      "12000 records processed of 12731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\438673665.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records processed of 10312\n",
      "1000 records processed of 10312\n",
      "2000 records processed of 10312\n",
      "3000 records processed of 10312\n",
      "4000 records processed of 10312\n",
      "5000 records processed of 10312\n",
      "6000 records processed of 10312\n",
      "7000 records processed of 10312\n",
      "8000 records processed of 10312\n",
      "9000 records processed of 10312\n",
      "10000 records processed of 10312\n",
      "Process executed in 35.69456505775452. 10 rows of 92086 dropped due to NaN / None values (0.01%).\n"
     ]
    }
   ],
   "source": [
    "# Replicates the big_table creating process on a much smaller scale\n",
    "# (also doesn't save it to file)\n",
    "\n",
    "start_time = time.time()\n",
    "for i in os.listdir(\"test_sets\"):\n",
    "\tos.remove(\"test_sets/\" + i)\n",
    "dropped_rows = 0\n",
    "total_rows = 0\n",
    "\n",
    "for i in os.listdir(\"test_data\"):\n",
    "\tticker_code = i.replace(\".csv\", \"\")\n",
    "\tlittle_table = pd.read_csv(\"test_data/\" + i)\n",
    "\tlittle_table[\"Close_next_day\"] = pd.Series(little_table[\"Close\"].shift(-1))\n",
    "\t# bands = pta.bbands(close = little_table[\"Close\"], length=20, std=2, mamode=\"ema\")[[\"BBL_20_2.0\", \"BBU_20_2.0\"]]\n",
    "\t# little_table[\"lower_bb\"], little_table[\"upper_bb\"] = bands.iloc[:,0], bands.iloc[:,1]\n",
    "\t# little_table[\"sma20\"] = pta.sma(little_table[\"Close\"], length=20)\n",
    "\t# little_table[\"sma50\"] = pta.sma(little_table[\"Close\"], length=50)\n",
    "\t# little_table[\"accdist\"] = pta.ad(high=little_table[\"High\"], low=little_table[\"Low\"], close=little_table[\"Close\"], volume=little_table[\"Volume\"])\n",
    "\t# little_table[\"adx\"] = pta.adx(high=little_table[\"High\"], low=little_table[\"Low\"], close = little_table[\"Close\"], length=30).iloc[:,0]\n",
    "\t# little_table[\"aroon\"] = pta.aroon(little_table[\"Close\"], little_table[\"Low\"], length=30).iloc[:,2]\n",
    "\t# little_table[\"macd\"] = pta.apo(little_table[\"Close\"]) #apo and macd are equivalent and this function is simpler\n",
    "\t# little_table[\"obv\"] = pta.obv(close=little_table[\"Close\"], volume=little_table[\"Volume\"])\n",
    "\t# little_table[\"rsi\"] = pta.rsi(close=little_table[\"Close\"], length=30)\n",
    "\t# little_table[\"stoch_o\"] = pta.stoch(high=little_table[\"High\"], low=little_table[\"Low\"], close=little_table[\"Close\"]).iloc[:,0]\n",
    "\tfor i in range(len(little_table)):\n",
    "\t\tlittle_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n",
    "\t\tif (i % 1000 == 0):\n",
    "\t\t\tprint(f\"{i} records processed of {len(little_table)}\")\n",
    "\tlittle_table[\"Date\"] = pd.to_numeric(little_table[\"Date\"])\n",
    "\tlittle_table[\"Volume\"] = pd.to_numeric(little_table[\"Volume\"])\n",
    "\told_len = len(little_table)\n",
    "\ttotal_rows += old_len # counts how many rows were dropped from the original dataset\n",
    "\tlittle_table = little_table.dropna()\n",
    "\tdropped_rows += (old_len - len(little_table)) #counts how many rows are dropped\n",
    "\tfeatures = little_table.drop(columns=[\"Close_next_day\", ])\n",
    "\tlabels = little_table[\"Close_next_day\"]\n",
    "\tscaler = MinMaxScaler()\n",
    "\tfeatures_norm = pd.DataFrame(scaler.fit_transform(features))\n",
    "\tX_tr, X_te, y_tr, y_te = train_test_split(features_norm, labels, train_size=0.8, shuffle=False)\n",
    "\tpd.DataFrame(X_tr).to_pickle(\"test_sets/\" + ticker_code + \"_X_train.pkl\")\n",
    "\tpd.DataFrame(X_te).to_pickle(\"test_sets/\" + ticker_code + \"_X_test.pkl\")\n",
    "\tpd.DataFrame(y_tr).to_pickle(\"test_sets/\" + ticker_code + \"_y_train.pkl\")\n",
    "\tpd.DataFrame(y_te).to_pickle(\"test_sets/\" + ticker_code + \"_y_test.pkl\")\n",
    "print(f\"Process executed in {time.time() - start_time}. {dropped_rows} rows of {total_rows} dropped due to NaN / None values ({round((dropped_rows / total_rows * 100), 2)}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\302303740.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records processed of 5791\n",
      "1000 records processed of 5791\n",
      "2000 records processed of 5791\n",
      "3000 records processed of 5791\n",
      "4000 records processed of 5791\n",
      "5000 records processed of 5791\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[24.71111298 26.92976189 24.48160744 ... 66.14961243 67.22248077\n 65.69535828].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [229], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m X_test \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n\u001b[0;32m     29\u001b[0m y_scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 30\u001b[0m y_train \u001b[39m=\u001b[39m y_scaler\u001b[39m.\u001b[39;49mfit_transform(y_train)\n\u001b[0;32m     31\u001b[0m y_test \u001b[39m=\u001b[39m y_test\u001b[39m.\u001b[39mtransform(y_test)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:420\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:457\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMinMaxScaler does not support sparse input. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using MaxAbsScaler instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    456\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 457\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    458\u001b[0m     X,\n\u001b[0;32m    459\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_pass,\n\u001b[0;32m    460\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    461\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m data_min \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmin(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    465\u001b[0m data_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmax(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[24.71111298 26.92976189 24.48160744 ... 66.14961243 67.22248077\n 65.69535828].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "i = \"A.csv\"\n",
    "little_table = pd.read_csv(\"test_data/\" + i)\n",
    "little_table[\"Close_next_day\"] = pd.Series(little_table[\"Close\"].shift(-1))\n",
    "# bands = pta.bbands(close = little_table[\"Close\"], length=20, std=2, mamode=\"ema\")[[\"BBL_20_2.0\", \"BBU_20_2.0\"]]\n",
    "# little_table[\"lower_bb\"], little_table[\"upper_bb\"] = bands.iloc[:,0], bands.iloc[:,1]\n",
    "# little_table[\"sma20\"] = pta.sma(little_table[\"Close\"], length=20)\n",
    "# little_table[\"sma50\"] = pta.sma(little_table[\"Close\"], length=50)\n",
    "# little_table[\"accdist\"] = pta.ad(high=little_table[\"High\"], low=little_table[\"Low\"], close=little_table[\"Close\"], volume=little_table[\"Volume\"])\n",
    "# little_table[\"adx\"] = pta.adx(high=little_table[\"High\"], low=little_table[\"Low\"], close = little_table[\"Close\"], length=30).iloc[:,0]\n",
    "# little_table[\"aroon\"] = pta.aroon(little_table[\"Close\"], little_table[\"Low\"], length=30).iloc[:,2]\n",
    "# little_table[\"macd\"] = pta.apo(little_table[\"Close\"]) #apo and macd are equivalent and this function is simpler\n",
    "# little_table[\"obv\"] = pta.obv(close=little_table[\"Close\"], volume=little_table[\"Volume\"])\n",
    "# little_table[\"rsi\"] = pta.rsi(close=little_table[\"Close\"], length=30)\n",
    "# little_table[\"stoch_o\"] = pta.stoch(high=little_table[\"High\"], low=little_table[\"Low\"], close=little_table[\"Close\"]).iloc[:,0]\n",
    "for i in range(len(little_table)):\n",
    "\tlittle_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n",
    "\tif (i % 1000 == 0):\n",
    "\t\tprint(f\"{i} records processed of {len(little_table)}\")\n",
    "# little_table[\"Date\"] = pd.to_numeric(little_table[\"Date\"])\n",
    "little_table[\"Volume\"] = pd.to_numeric(little_table[\"Volume\"])\n",
    "X_train = little_table[[\"Date\", \"Close\"]]\n",
    "X_test = X_train[int(len(X_train) * 0.8):]\n",
    "X_train = X_train[:int(len(X_train) * 0.8)]\n",
    "y_train = little_table[\"Close_next_day\"].iloc[:len(X_train)]\n",
    "y_test = little_table[\"Close_next_day\"].iloc[len(X_train):]\n",
    "x_scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "y_test = y_test.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X ->'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Y ->'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"X ->\", len(X_test) + len(X_train) == len(little_table))\n",
    "display(\"Y ->\", len(y_test) + len(y_train) == len(little_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.69535827636719"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.iloc[len(X_train) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.28939819335938"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 1.24287480e-01, 1.38803465e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.05504427e-01],\n",
       "       [1.18976800e-04, 1.15135536e-01, 1.13806666e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.18458002e-01],\n",
       "       [4.75907198e-04, 1.09331912e-01, 1.17377659e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.04164460e-01],\n",
       "       ...,\n",
       "       [9.99524093e-01, 8.17717092e-01, 8.23532293e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.08628075e-01],\n",
       "       [9.99881023e-01, 8.11531615e-01, 8.18864651e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 8.77055262e-01],\n",
       "       [1.00000000e+00, 8.48644567e-01, 8.78843616e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00,            nan]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "little_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(test_set, steps):\n",
    "\tX_list = []\n",
    "\tfunction_set = test_set.copy()\n",
    "\tfor i in range(len(test_set) - steps):\n",
    "\t\tx_slice = function_set[0:steps]\n",
    "\t\tX_list.append(x_slice)\n",
    "\t\tfunction_set = function_set.shift(-1)\n",
    "\tdel(function_set)\n",
    "\treturn(np.array(X_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 2.23759734e-01],\n",
       "       [1.48676777e-04, 1.99291245e-01],\n",
       "       [5.94707107e-04, 2.23759734e-01],\n",
       "       ...,\n",
       "       [9.99702646e-01, 6.53420004e-01],\n",
       "       [9.99851323e-01, 6.56297969e-01],\n",
       "       [1.00000000e+00, 6.68130155e-01]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 3\n",
    "X_list = create_windows(pd.DataFrame(X_train[:,1]), steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "145/145 [==============================] - 5s 5ms/step - loss: 19.0094\n",
      "Epoch 2/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 10.2802\n",
      "Epoch 3/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 10.0083\n",
      "Epoch 4/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 9.8779\n",
      "Epoch 5/50\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 7.4490\n",
      "Epoch 6/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 4.7544\n",
      "Epoch 7/50\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 3.7513\n",
      "Epoch 8/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 3.2235\n",
      "Epoch 9/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 2.7553\n",
      "Epoch 10/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 2.3818\n",
      "Epoch 11/50\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 2.0860\n",
      "Epoch 12/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.8647\n",
      "Epoch 13/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.7055\n",
      "Epoch 14/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.6410\n",
      "Epoch 15/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.5665\n",
      "Epoch 16/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.5238\n",
      "Epoch 17/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.4754\n",
      "Epoch 18/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.4839\n",
      "Epoch 19/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.4423\n",
      "Epoch 20/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.4220\n",
      "Epoch 21/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.3971\n",
      "Epoch 22/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.4017\n",
      "Epoch 23/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.3718\n",
      "Epoch 24/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.3529\n",
      "Epoch 25/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.3237\n",
      "Epoch 26/50\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 1.3292\n",
      "Epoch 27/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.3066\n",
      "Epoch 28/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2853\n",
      "Epoch 29/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2615\n",
      "Epoch 30/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2239\n",
      "Epoch 31/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2499\n",
      "Epoch 32/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2203\n",
      "Epoch 33/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2211\n",
      "Epoch 34/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2412\n",
      "Epoch 35/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2217\n",
      "Epoch 36/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2001\n",
      "Epoch 37/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1987\n",
      "Epoch 38/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1876\n",
      "Epoch 39/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1661\n",
      "Epoch 40/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.2081\n",
      "Epoch 41/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1504\n",
      "Epoch 42/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1584\n",
      "Epoch 43/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1581\n",
      "Epoch 44/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1370\n",
      "Epoch 45/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1476\n",
      "Epoch 46/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1321\n",
      "Epoch 47/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1326\n",
      "Epoch 48/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1217\n",
      "Epoch 49/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1209\n",
      "Epoch 50/50\n",
      "145/145 [==============================] - 1s 6ms/step - loss: 1.1294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x293a5924ac0>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "n_features = 1\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, activation='relu', input_shape=(steps, n_features), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='huber')\n",
    "model.fit(X_list, y_train[:(steps * -1)], epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1159,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1156, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_29184\\15072840.py:2: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  model.evaluate(X_test_list, y_test[:(steps * -1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 1s 2ms/step - loss: 30.9408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30.940793991088867"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(y_test.shape, X_test_list.shape)\n",
    "model.evaluate(X_test_list, y_test[:(steps * -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_54 (LSTM)              (None, 3, 64)             16896     \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 3, 64)             0         \n",
      "                                                                 \n",
      " lstm_55 (LSTM)              (None, 3, 64)             33024     \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 3, 64)             0         \n",
      "                                                                 \n",
      " lstm_56 (LSTM)              (None, 3, 64)             33024     \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 3, 64)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 3, 1)              65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83,009\n",
      "Trainable params: 83,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('final-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "937ac2121f4bd33c921bc04ce0336a5d7e4eb91bbd651e21166683a68e8f5aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
