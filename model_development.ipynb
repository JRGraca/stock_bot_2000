{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta as pta\n",
    "import ta\n",
    "# from datapackage import Package\n",
    "# from selenium import webdriver\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import yfinance as yf\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import graphviz\n",
    "import pydot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to slice the data according to needs\n",
    "def create_windows(test_set, steps):\n",
    "\tX_list = []\n",
    "\tfunction_set = test_set.copy()\n",
    "\tfor i in range(len(test_set) - steps):\n",
    "\t\tx_slice = function_set[0:steps]\n",
    "\t\tX_list.append(x_slice)\n",
    "\t\tfunction_set = function_set.shift(-1)\n",
    "\tdel(function_set)\n",
    "\treturn(np.array(X_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_27408\\2509814590.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records processed of 5791\n",
      "1000 records processed of 5791\n",
      "2000 records processed of 5791\n",
      "3000 records processed of 5791\n",
      "4000 records processed of 5791\n",
      "5000 records processed of 5791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BogPoet\\AppData\\Local\\Temp\\ipykernel_27408\\2509814590.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  little_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records processed of 4320\n",
      "1000 records processed of 4320\n",
      "2000 records processed of 4320\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m little_table[\u001b[39m\"\u001b[39m\u001b[39mstoch_o\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pta\u001b[39m.\u001b[39mstoch(high\u001b[39m=\u001b[39mlittle_table[\u001b[39m\"\u001b[39m\u001b[39mHigh\u001b[39m\u001b[39m\"\u001b[39m], low\u001b[39m=\u001b[39mlittle_table[\u001b[39m\"\u001b[39m\u001b[39mLow\u001b[39m\u001b[39m\"\u001b[39m], close\u001b[39m=\u001b[39mlittle_table[\u001b[39m\"\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39miloc[:,\u001b[39m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(little_table)):\n\u001b[1;32m---> 31\u001b[0m \tlittle_table[\u001b[39m\"\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[i] \u001b[39m=\u001b[39m (pd\u001b[39m.\u001b[39mto_datetime(little_table[\u001b[39m\"\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[i][:\u001b[39m10\u001b[39m]))\u001b[39m.\u001b[39mtoordinal() \u001b[39m#Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \t\u001b[39mif\u001b[39;00m (i \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m     33\u001b[0m \t\t\u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m records processed of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(little_table)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\indexing.py:1797\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1796\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1797\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\indexing.py:2075\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   2072\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_align_frame(indexer, value)\n\u001b[0;32m   2074\u001b[0m \u001b[39m# check for chained assignment\u001b[39;00m\n\u001b[1;32m-> 2075\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_check_is_chained_assignment_possible()\n\u001b[0;32m   2077\u001b[0m \u001b[39m# actually do the set\u001b[39;00m\n\u001b[0;32m   2078\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39msetitem(indexer\u001b[39m=\u001b[39mindexer, value\u001b[39m=\u001b[39mvalue)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\series.py:1284\u001b[0m, in \u001b[0;36mSeries._check_is_chained_assignment_possible\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_view \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_cached:\n\u001b[0;32m   1283\u001b[0m     ref \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_cacher()\n\u001b[1;32m-> 1284\u001b[0m     \u001b[39mif\u001b[39;00m ref \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ref\u001b[39m.\u001b[39;49m_is_mixed_type:\n\u001b[0;32m   1285\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_setitem_copy(t\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreferent\u001b[39m\u001b[39m\"\u001b[39m, force\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1286\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\generic.py:6007\u001b[0m, in \u001b[0;36mNDFrame._is_mixed_type\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6002\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39many_extension_types:\n\u001b[0;32m   6003\u001b[0m     \u001b[39m# Even if they have the same dtype, we can't consolidate them,\u001b[39;00m\n\u001b[0;32m   6004\u001b[0m     \u001b[39m#  so we pretend this is \"mixed'\"\u001b[39;00m\n\u001b[0;32m   6005\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m-> 6007\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtypes\u001b[39m.\u001b[39;49mnunique() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\base.py:1039\u001b[0m, in \u001b[0;36mIndexOpsMixin.nunique\u001b[1;34m(self, dropna)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnunique\u001b[39m(\u001b[39mself\u001b[39m, dropna: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m   1006\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[39m    Return number of unique elements in the object.\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[39m    4\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1039\u001b[0m     uniqs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munique()\n\u001b[0;32m   1040\u001b[0m     \u001b[39mif\u001b[39;00m dropna:\n\u001b[0;32m   1041\u001b[0m         uniqs \u001b[39m=\u001b[39m remove_na_arraylike(uniqs)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\series.py:2242\u001b[0m, in \u001b[0;36mSeries.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2183\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munique\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[0;32m   2184\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2185\u001b[0m \u001b[39m    Return unique values of Series object.\u001b[39;00m\n\u001b[0;32m   2186\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2240\u001b[0m \u001b[39m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[0;32m   2241\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2242\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49munique()\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\base.py:1001\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    999\u001b[0m         result \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(result)\n\u001b[0;32m   1000\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1001\u001b[0m     result \u001b[39m=\u001b[39m unique1d(values)\n\u001b[0;32m   1003\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\algorithms.py:409\u001b[0m, in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munique\u001b[39m(values):\n\u001b[0;32m    316\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[39m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     \u001b[39mreturn\u001b[39;00m unique_with_mask(values)\n",
      "File \u001b[1;32md:\\ProgramFiles\\Anaconda\\envs\\final-project\\lib\\site-packages\\pandas\\core\\algorithms.py:425\u001b[0m, in \u001b[0;36munique_with_mask\u001b[1;34m(values, mask)\u001b[0m\n\u001b[0;32m    423\u001b[0m table \u001b[39m=\u001b[39m htable(\u001b[39mlen\u001b[39m(values))\n\u001b[0;32m    424\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 425\u001b[0m     uniques \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39;49munique(values)\n\u001b[0;32m    426\u001b[0m     uniques \u001b[39m=\u001b[39m _reconstruct_data(uniques, original\u001b[39m.\u001b[39mdtype, original)\n\u001b[0;32m    427\u001b[0m     \u001b[39mreturn\u001b[39;00m uniques\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "dropped_rows = 0\n",
    "total_rows = 0\n",
    "steps = 7\n",
    "\n",
    "for folder in [\"X_test\", \"X_train\", \"y_test\", \"y_train\"]:\n",
    "\tfor file in os.listdir(\"sets/\" + folder):\n",
    "\t\tos.remove(\"sets/\" + folder + \"/\" + file)\n",
    "\n",
    "for file in os.listdir(\"models\"):\n",
    "\tos.remove(\"models/\" + file)\t\n",
    "\n",
    "for i in os.listdir(\"data\"):\n",
    "\tticker_code = i.replace(\".csv\", \"\")\n",
    "\tlittle_table = pd.read_csv(\"data/\" + i)\n",
    "\tlittle_table[\"Close_next_day\"] = pd.Series(little_table[\"Close\"].shift(-1))\n",
    "\tbands = pta.bbands(close = little_table[\"Close\"], length=20, std=2, mamode=\"ema\")[[\"BBL_20_2.0\", \"BBU_20_2.0\"]]\n",
    "\tlittle_table[\"lower_bb\"], little_table[\"upper_bb\"] = bands.iloc[:,0], bands.iloc[:,1]\n",
    "\tlittle_table[\"sma20\"] = pta.sma(little_table[\"Close\"], length=20)\n",
    "\tlittle_table[\"sma50\"] = pta.sma(little_table[\"Close\"], length=50)\n",
    "\tlittle_table[\"accdist\"] = pta.ad(high=little_table[\"High\"], low=little_table[\"Low\"], close=little_table[\"Close\"], volume=little_table[\"Volume\"])\n",
    "\tlittle_table[\"adx\"] = pta.adx(high=little_table[\"High\"], low=little_table[\"Low\"], close = little_table[\"Close\"], length=30).iloc[:,0]\n",
    "\tlittle_table[\"aroon\"] = pta.aroon(little_table[\"Close\"], little_table[\"Low\"], length=30).iloc[:,2]\n",
    "\tlittle_table[\"macd\"] = pta.apo(little_table[\"Close\"]) #apo and macd are equivalent and this function is simpler\n",
    "\tlittle_table[\"obv\"] = pta.obv(close=little_table[\"Close\"], volume=little_table[\"Volume\"])\n",
    "\tlittle_table[\"rsi\"] = pta.rsi(close=little_table[\"Close\"], length=30)\n",
    "\tlittle_table[\"stoch_o\"] = pta.stoch(high=little_table[\"High\"], low=little_table[\"Low\"], close=little_table[\"Close\"]).iloc[:,0]\n",
    "\tfor i in range(len(little_table)):\n",
    "\t\tlittle_table[\"Date\"].iloc[i] = (pd.to_datetime(little_table[\"Date\"].iloc[i][:10])).toordinal() #Should probably have breaken this down but it converts the string of Date to date and then to numeric (ordinal) for use with TensorFlow\n",
    "\t\tif (i % 1000 == 0):\n",
    "\t\t\tprint(f\"{i} records processed of {len(little_table)}\")\n",
    "\tlittle_table[\"Date\"] = pd.to_numeric(little_table[\"Date\"])\n",
    "\tlittle_table[\"Volume\"] = pd.to_numeric(little_table[\"Volume\"])\n",
    "\told_len = len(little_table)\n",
    "\ttotal_rows += old_len # counts how many rows were dropped from the original dataset\n",
    "\tlittle_table = little_table.dropna()\n",
    "\tdropped_rows += (old_len - len(little_table)) #counts how many rows are dropped\n",
    "\tfeatures = little_table.drop(columns=[\"Close_next_day\", ])\n",
    "\tlabels = little_table[\"Close_next_day\"]\n",
    "\tscaler = MinMaxScaler()\n",
    "\tX_tr, X_te, y_tr, y_te = train_test_split(features, labels, train_size=0.8, shuffle=False)\n",
    "\tscaler.fit_transform(X_tr)\n",
    "\tscaler.transform(X_te)\t\n",
    "\tpd.DataFrame(X_tr).to_pickle(\"sets/X_train/\" + ticker_code + \"_X_train.pkl\")\n",
    "\tpd.DataFrame(X_te).to_pickle(\"sets/X_test/\" + ticker_code + \"_X_test.pkl\")\n",
    "\tpd.DataFrame(y_tr).to_pickle(\"sets/y_train/\" + ticker_code + \"_y_train.pkl\")\n",
    "\tpd.DataFrame(y_te).to_pickle(\"sets/y_test/\" + ticker_code + \"_y_test.pkl\")\n",
    "\n",
    "for i in os.listdir(\"sets/X_train\"):\n",
    "\tticker_code = i.replace(\"_X_train.pkl\", \"\")\n",
    "\tfeatures = pd.read_pickle(\"sets/X_train/\" + ticker_code + \"_X_train.pkl\")\n",
    "\tlabel = pd.read_pickle(\"sets/y_train/\" + ticker_code + \"_y_train.pkl\")\n",
    "\tsteps = 5\n",
    "\tX_list = create_windows(features, steps)\n",
    "\tn_features = X_list.shape[2]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(64, activation='relu', input_shape=(steps, n_features), return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(units=1))\n",
    "\tmodel.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\tmodel.fit(X_list, label[:(steps * -1)], epochs=20)\n",
    "\tX_test = pd.read_pickle(\"sets/X_test/\" + ticker_code + \"_X_test.pkl\")\n",
    "\ty_test = pd.read_pickle(\"sets/y_test/\" + ticker_code + \"_y_test.pkl\")\n",
    "\tX_test_win = create_windows(X_test, steps)\n",
    "\tscore = model.evaluate(X_test_win, y_test[:(steps * -1)])\n",
    "\tpd.to_pickle(model, \"models/\" + ticker_code + \"_score_\" + str(round(score, 2)) + \".pkl\")\n",
    "print(f\"Process executed in {(time.time() - start_time) / 3600} hours. {dropped_rows} rows of {total_rows} dropped due to NaN / None values ({round((dropped_rows / total_rows * 100), 2)}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 3\n",
    "\n",
    "for i in os.listdir(\"sets/X_train\"):\n",
    "\tticker_code = i.replace(\"_X_train.pkl\", \"\")\n",
    "\tfeatures = pd.read_pickle(\"sets/X_train/\" + ticker_code + \"_X_train.pkl\")\n",
    "\tlabel = pd.read_pickle(\"sets/y_train/\" + ticker_code + \"_y_train.pkl\")\n",
    "\tsteps = 5\n",
    "\tX_list = create_windows(features, steps)\n",
    "\tn_features = X_list.shape[2]\n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(64, activation='relu', input_shape=(steps, n_features), return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64, return_sequences=True))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(LSTM(64))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Dense(units=1))\n",
    "\tmodel.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\tmodel.fit(X_list, label[:(steps * -1)], epochs=20)\n",
    "\tX_test = pd.read_pickle(\"sets/X_test/\" + ticker_code + \"_X_test.pkl\")\n",
    "\ty_test = pd.read_pickle(\"sets/y_test/\" + ticker_code + \"_y_test.pkl\")\n",
    "\tX_test_win = create_windows(X_test, steps)\n",
    "\tscore = model.evaluate(X_test_win, y_test[:(steps * -1)])\n",
    "\tpd.to_pickle(model, \"models/\" + ticker_code + \"_score_\" + str(round(score, 2)) + \".pkl\")\n",
    "print(f\"Process executed in {(time.time() - start_time) / 3600} hours. {dropped_rows} rows of {total_rows} dropped due to NaN / None values ({round((dropped_rows / total_rows * 100), 2)}%).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-11-28 05:37:48         7852\n",
      "metadata.json                                  2022-11-28 05:37:48           64\n",
      "variables.h5                                   2022-11-28 05:37:48      2710000\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...layers\\dropout\n",
      "......vars\n",
      "...layers\\dropout_1\n",
      "......vars\n",
      "...layers\\dropout_2\n",
      "......vars\n",
      "...layers\\dropout_3\n",
      "......vars\n",
      "...layers\\dropout_4\n",
      "......vars\n",
      "...layers\\dropout_5\n",
      "......vars\n",
      "...layers\\dropout_6\n",
      "......vars\n",
      "...layers\\lstm\n",
      "......vars\n",
      "...layers\\lstm\\cell\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...layers\\lstm_1\n",
      "......vars\n",
      "...layers\\lstm_1\\cell\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...layers\\lstm_2\n",
      "......vars\n",
      "...layers\\lstm_2\\cell\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...layers\\lstm_3\n",
      "......vars\n",
      "...layers\\lstm_3\\cell\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...layers\\lstm_4\n",
      "......vars\n",
      "...layers\\lstm_4\\cell\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...layers\\lstm_5\n",
      "......vars\n",
      "...layers\\lstm_5\\cell\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...layers\\lstm_6\n",
      "......vars\n",
      "...layers\\lstm_6\\cell\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........29\n",
      ".........3\n",
      ".........30\n",
      ".........31\n",
      ".........32\n",
      ".........33\n",
      ".........34\n",
      ".........35\n",
      ".........36\n",
      ".........37\n",
      ".........38\n",
      ".........39\n",
      ".........4\n",
      ".........40\n",
      ".........41\n",
      ".........42\n",
      ".........43\n",
      ".........44\n",
      ".........45\n",
      ".........46\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "test_model = pd.read_pickle(\"models/AVGO_score_140789.06.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('final-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "937ac2121f4bd33c921bc04ce0336a5d7e4eb91bbd651e21166683a68e8f5aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
